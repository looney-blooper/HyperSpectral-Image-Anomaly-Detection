{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a90904d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-13 11:03:45.100339: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-11-13 11:03:45.234007: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-11-13 11:03:48.205359: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patches shape: (1, 36, 3, 3, 5) info: {'new_h': 6, 'new_w': 6, 'ph': 3, 'pw': 3, 'sh': 3, 'sw': 3, 'padding': 'SAME'}\n",
      "max abs diff: 0.0\n",
      "Round-trip OK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "W0000 00:00:1763012030.204683   20983 gpu_device.cc:2342] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    }
   ],
   "source": [
    "# tests/test_block_roundtrip.py\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from model.block_tf import BlockEmbeddingTF, BlockFoldTF\n",
    "\n",
    "def run_test():\n",
    "    # small synthetic image\n",
    "    B = 1\n",
    "    H = 18\n",
    "    W = 18\n",
    "    C = 5\n",
    "    img = np.random.rand(B, H, W, C).astype(np.float32)\n",
    "    images = tf.convert_to_tensor(img)\n",
    "\n",
    "    patch_h = 3\n",
    "    patch_w = 3\n",
    "    stride = 3\n",
    "\n",
    "    be = BlockEmbeddingTF(patch_h=patch_h, patch_w=patch_w, stride_h=stride, stride_w=stride, padding='SAME')\n",
    "    patches, info = be.extract(images)\n",
    "    print(\"patches shape:\", patches.shape, \"info:\", info)\n",
    "\n",
    "    bf = BlockFoldTF()\n",
    "    recon = bf.fold(patches, info, orig_H=H, orig_W=W)\n",
    "    recon_np = recon.numpy()\n",
    "\n",
    "    # compare only the central region that is within valid reconstruction (tolerance)\n",
    "    diff = np.abs(recon_np - img)\n",
    "    print(\"max abs diff:\", diff.max())\n",
    "    assert diff.max() < 1e-5 or diff.max() < 1e-3, \"Round-trip error too large\"\n",
    "    print(\"Round-trip OK\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    run_test()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5c29c17f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "W0000 00:00:1763033796.713866   25298 gpu_device.cc:2342] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attn_afb shape: (2, 9, 9)\n",
      "AFB diagonal mean: 0.0\n",
      "BFB diagonal mean: 0.8623292\n",
      "diag_afb mean: 0.0\n",
      "diag_bfb mean: 0.8623291850090027\n"
     ]
    }
   ],
   "source": [
    "# tests/test_attention_tf.py\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from model.attention_tf import AttentionBlock as AttentionTF\n",
    "\n",
    "def demo():\n",
    "    tf.random.set_seed(0)\n",
    "    B = 2\n",
    "    psize = 3\n",
    "    pstride = 3\n",
    "    H = psize * pstride\n",
    "    W = H\n",
    "    C = 8\n",
    "    x = tf.random.normal((B, H, W, C), dtype=tf.float32)\n",
    "\n",
    "    # create attention module\n",
    "    att = AttentionTF(embed_dim=C, patch_size=psize, patch_stride=pstride, proj_ratio=4, attn_drop=0.0)\n",
    "    # build by calling once\n",
    "    dummy = att(x, block_idx=tf.constant([0,1], dtype=tf.int32), match_vec=tf.constant([0,0,0,0], dtype=tf.float32), return_attn=False)\n",
    "\n",
    "    # Case 1: match_vec all zeros -> AFB behavior (self suppressed)\n",
    "    num_blocks_global = 10\n",
    "    match_vec = tf.zeros([num_blocks_global], dtype=tf.float32)\n",
    "    block_idx = tf.constant([0, 1], dtype=tf.int32)  # pretend these are indices into match_vec\n",
    "    out_afb, attn_afb = att(x, block_idx=block_idx, match_vec=match_vec, return_attn=True, training=False)\n",
    "    print(\"attn_afb shape:\", attn_afb.shape)  # [B, N, N]\n",
    "    # inspect diagonal mean\n",
    "    diag_afb = tf.linalg.diag_part(attn_afb)  # [B, N]\n",
    "    print(\"AFB diagonal mean:\", tf.reduce_mean(diag_afb).numpy())\n",
    "\n",
    "    # Case 2: match_vec has ones -> BFB behavior (for those batch samples)\n",
    "    match_vec2 = tf.ones([num_blocks_global], dtype=tf.float32)\n",
    "    out_bfb, attn_bfb = att(x, block_idx=block_idx, match_vec=match_vec2, return_attn=True, training=False)\n",
    "    diag_bfb = tf.linalg.diag_part(attn_bfb)\n",
    "    print(\"BFB diagonal mean:\", tf.reduce_mean(diag_bfb).numpy())\n",
    "\n",
    "    # Expect diag_afb mean << diag_bfb mean\n",
    "    print(\"diag_afb mean:\", float(tf.reduce_mean(diag_afb)))\n",
    "    print(\"diag_bfb mean:\", float(tf.reduce_mean(diag_bfb)))\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    demo()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e6708387",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in shape: (4, 9, 9, 8) out shape: (4, 9, 9, 8)\n",
      "GTB/Net forward OK\n"
     ]
    }
   ],
   "source": [
    "# tests/test_gtb_net.py\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from model.gtblock_tf import NetTF\n",
    "\n",
    "def run_test():\n",
    "    B = 4\n",
    "    psize = 3\n",
    "    pstride = 3\n",
    "    H = psize * pstride\n",
    "    W = H\n",
    "    in_chans = 8\n",
    "    embed_dim = 16\n",
    "\n",
    "    # create random blocks in channels-last\n",
    "    x = tf.random.normal((B, H, W, in_chans), dtype=tf.float32)\n",
    "    # dummy match_vec and block_idx\n",
    "    num_blocks_global = 100\n",
    "    match_vec = tf.zeros([num_blocks_global], dtype=tf.float32)\n",
    "    block_idx = tf.constant([0,1,2,3], dtype=tf.int32)\n",
    "\n",
    "    net = NetTF(in_chans=in_chans, embed_dim=embed_dim, patch_size=psize, patch_stride=pstride, mlp_ratio=2.0, proj_ratio=4)\n",
    "    out = net(x, block_idx=block_idx, match_vec=match_vec, training=True)\n",
    "    print(\"in shape:\", x.shape, \"out shape:\", out.shape)\n",
    "    assert out.shape == x.shape\n",
    "    print(\"GTB/Net forward OK\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    run_test()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5db6f7bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-13 18:58:14.846943: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-11-13 18:58:15.167497: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-11-13 18:58:16.633807: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'block_tf'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\u001b[38;5;241m,\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmodel\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mblock_tf\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BlockEmbeddingTF, BlockFoldTF\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmodel\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mblock_search_tf\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BlockSearchTF\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mrun_test\u001b[39m():\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;66;03m# synthetic image\u001b[39;00m\n\u001b[1;32m      8\u001b[0m     H \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m18\u001b[39m; W \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m18\u001b[39m; C \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m4\u001b[39m\n",
      "File \u001b[0;32m~/ghostdrive/my_projects/GT-HAD/model/block_search_tf.py:3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# src/block_search_tf.py\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mblock_tf\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BlockEmbeddingTF   \u001b[38;5;66;03m# your extractor (channels-last)\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mBlockSearchTF\u001b[39;00m:\n\u001b[1;32m      6\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;124;03m    Compute match_vec for GT-HAD using folded reconstructions.\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;124;03m    Works with BlockFoldTF that assumes no-overlap (sh==ph, sw==pw).\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'block_tf'"
     ]
    }
   ],
   "source": [
    "# tests/test_block_search_integration.py\n",
    "import numpy as np, tensorflow as tf\n",
    "from model.block_tf import BlockEmbeddingTF, BlockFoldTF\n",
    "from model.block_search_tf import BlockSearchTF\n",
    "\n",
    "def run_test():\n",
    "    # synthetic image\n",
    "    H = 18; W = 18; C = 4\n",
    "    rng = np.random.RandomState(2)\n",
    "    img = rng.rand(H, W, C).astype(np.float32)\n",
    "    img_tf = tf.expand_dims(tf.convert_to_tensor(img), axis=0)  # [1,H,W,C]\n",
    "\n",
    "    # extractor params (no-overlap assumption: stride == patch size)\n",
    "    ph = 3; pw = 3; sh = 3; sw = 3\n",
    "    be = BlockEmbeddingTF(patch_h=ph, patch_w=pw, stride_h=sh, stride_w=sw, padding='SAME')\n",
    "    patches_orig, info = be.extract(img_tf)  # [1, N, ph, pw, C]\n",
    "    N = patches_orig.shape[1]\n",
    "    block_query = tf.reshape(patches_orig[0], (N, -1))  # [N, L]\n",
    "\n",
    "    # simulate search_matrix == original reconstructions\n",
    "    search_matrix = patches_orig[0].numpy()  # [N, ph, pw, C]\n",
    "    # wrap as batched:\n",
    "    search_batched = np.expand_dims(search_matrix, axis=0)  # [1,N,ph,pw,C]\n",
    "\n",
    "    bf = BlockFoldTF()\n",
    "    # fold back:\n",
    "    recon = bf.fold(tf.convert_to_tensor(search_batched), info, orig_H=H, orig_W=W)  # [1,H,W,C]\n",
    "\n",
    "    # run BlockSearchTF\n",
    "    bs = BlockSearchTF(block_embedding=be, block_query=block_query)\n",
    "    match_vec = bs.compute_match_vec_from_batched_search_matrix(search_batched, info, orig_H=H, orig_W=W)\n",
    "    print(\"match_vec sum (should be N):\", int(tf.reduce_sum(match_vec).numpy()), \"N:\", N)\n",
    "    assert int(tf.reduce_sum(match_vec).numpy()) == int(N)\n",
    "    print(\"BlockSearch integration test PASSED\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    run_test()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77c45dd9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
